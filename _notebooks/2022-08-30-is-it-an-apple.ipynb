{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# \"Is it an apple?  First model lecture 1 fastai 2022\"\n> \"Here's my first model from the first lecture.  I changed out the bird and forest to apples and oranges. ðŸ˜Š\"\n\n- toc:true- branch: master\n- badges: true\n- comments: false\n- categories: [fastpages, jupyter]","metadata":{}},{"cell_type":"markdown","source":"## Is it an apple?","metadata":{}},{"cell_type":"markdown","source":"Chapter 1 first model on apples and oranges.  Here I just replaced bird and forest with apple and oranges.  I did have to change search \"apple\" and \"orange\" into \"apple fruit\" and \"orange fruit\" to get more fruit pictures. It worked and the model did great! ","metadata":{}},{"cell_type":"code","source":"#NB: Kaggle requires phone verification to use the internet or a GPU. If you haven't done that yet, the cell below will fail\n#    This code is only here to check that your internet is enabled. It doesn't do anything else.\n#    Here's a help thread on getting your phone number verified: https://www.kaggle.com/product-feedback/135367\n\nimport socket,warnings\ntry:\n    socket.setdefaulttimeout(1)\n    socket.socket(socket.AF_INET, socket.SOCK_STREAM).connect(('1.1.1.1', 53))\nexcept socket.error as ex: raise Exception(\"STOP: No internet. Click '>|' in top right and set 'Internet' switch to on\")","metadata":{"execution":{"iopub.status.busy":"2022-08-30T16:31:21.815407Z","iopub.execute_input":"2022-08-30T16:31:21.815854Z","iopub.status.idle":"2022-08-30T16:31:21.840970Z","shell.execute_reply.started":"2022-08-30T16:31:21.815737Z","shell.execute_reply":"2022-08-30T16:31:21.840314Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# It's a good idea to ensure you're running the latest version of any libraries you need.\n# `!pip install -Uqq <libraries>` upgrades to the latest version of <libraries>\n# NB: You can safely ignore any warnings or errors pip spits out about running as root or incompatibilities\nimport os\niskaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')\n\nif iskaggle:\n    !pip install -Uqq fastai duckduckgo_search","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-08-30T16:31:25.266149Z","iopub.execute_input":"2022-08-30T16:31:25.266731Z","iopub.status.idle":"2022-08-30T16:31:47.382098Z","shell.execute_reply.started":"2022-08-30T16:31:25.266694Z","shell.execute_reply":"2022-08-30T16:31:47.381217Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"But today, we can do exactly that, in just a few minutes, using entirely free resources!\n\nThe basic steps we'll take are:\n\n1. Use DuckDuckGo to search for images of \"apple photos\"\n1. Use DuckDuckGo to search for images of \"oranges photos\"\n1. Fine-tune a pretrained neural network to recognise these two groups\n1. Try running this model on a picture of an apple and see if it works.","metadata":{}},{"cell_type":"markdown","source":"## Step 1: Download images of apples and oranges.","metadata":{}},{"cell_type":"code","source":"from duckduckgo_search import ddg_images\nfrom fastcore.all import *\n\ndef search_images(term, max_images=30):\n    print(f\"Searching for '{term}'\")\n    return L(ddg_images(term, max_results=max_images)).itemgot('image')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-08-30T16:38:53.370039Z","iopub.execute_input":"2022-08-30T16:38:53.370827Z","iopub.status.idle":"2022-08-30T16:38:53.376529Z","shell.execute_reply.started":"2022-08-30T16:38:53.370789Z","shell.execute_reply":"2022-08-30T16:38:53.375605Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"Let's start by searching for a bird photo and seeing what kind of result we get. We'll start by getting URLs from a search:","metadata":{}},{"cell_type":"code","source":"#NB: `search_images` depends on duckduckgo.com, which doesn't always return correct responses.\n#    If you get a JSON error, just try running it again (it may take a couple of tries).\nurls = search_images('apple fruit photos', max_images=1)\nurls[0]","metadata":{"execution":{"iopub.status.busy":"2022-08-30T16:43:45.875503Z","iopub.execute_input":"2022-08-30T16:43:45.876101Z","iopub.status.idle":"2022-08-30T16:43:48.274052Z","shell.execute_reply.started":"2022-08-30T16:43:45.876064Z","shell.execute_reply":"2022-08-30T16:43:48.273337Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"...and then download a URL and take a look at it:","metadata":{}},{"cell_type":"code","source":"from fastdownload import download_url\ndest = 'apple.jpg'\ndownload_url(urls[0], dest, show_progress=False)\n\nfrom fastai.vision.all import *\nim = Image.open(dest)\nim.to_thumb(256,256)","metadata":{"execution":{"iopub.status.busy":"2022-08-30T16:43:54.697023Z","iopub.execute_input":"2022-08-30T16:43:54.697681Z","iopub.status.idle":"2022-08-30T16:43:57.285088Z","shell.execute_reply.started":"2022-08-30T16:43:54.697625Z","shell.execute_reply":"2022-08-30T16:43:57.284254Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"Now let's do the same with \"orange photos\":","metadata":{}},{"cell_type":"code","source":"download_url(search_images('orange fruit photos', max_images=1)[0], 'orange.jpg', show_progress=False)\nImage.open('orange.jpg').to_thumb(256,256)","metadata":{"execution":{"iopub.status.busy":"2022-08-30T16:44:13.830094Z","iopub.execute_input":"2022-08-30T16:44:13.830816Z","iopub.status.idle":"2022-08-30T16:44:19.246451Z","shell.execute_reply.started":"2022-08-30T16:44:13.830779Z","shell.execute_reply":"2022-08-30T16:44:19.245620Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"Our searches seem to be giving reasonable results, so let's grab a few examples of each of \"apple\" and \"orange\" photos, and save each group of photos to a different folder (I'm also trying to grab a range of lighting conditions here):","metadata":{}},{"cell_type":"code","source":"searches = 'orange fruit','apple'\npath = Path('orange_or_not')\nfrom time import sleep\n\nfor o in searches:\n    dest = (path/o)\n    dest.mkdir(exist_ok=True, parents=True)\n    download_images(dest, urls=search_images(f'{o} photo'))\n    sleep(10)  # Pause between searches to avoid over-loading server\n    download_images(dest, urls=search_images(f'{o} sun photo'))\n    sleep(10)\n    download_images(dest, urls=search_images(f'{o} shade photo'))\n    sleep(10)\n    resize_images(path/o, max_size=400, dest=path/o)","metadata":{"execution":{"iopub.status.busy":"2022-08-30T16:44:34.993970Z","iopub.execute_input":"2022-08-30T16:44:34.994251Z","iopub.status.idle":"2022-08-30T16:46:43.975612Z","shell.execute_reply.started":"2022-08-30T16:44:34.994200Z","shell.execute_reply":"2022-08-30T16:46:43.974628Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"## Step 2: Train our model","metadata":{}},{"cell_type":"markdown","source":"Some photos might not download correctly which could cause our model training to fail, so we'll remove them:","metadata":{}},{"cell_type":"code","source":"failed = verify_images(get_image_files(path))\nfailed.map(Path.unlink)\nlen(failed)","metadata":{"execution":{"iopub.status.busy":"2022-08-30T16:46:45.022243Z","iopub.execute_input":"2022-08-30T16:46:45.023044Z","iopub.status.idle":"2022-08-30T16:46:45.895000Z","shell.execute_reply.started":"2022-08-30T16:46:45.023002Z","shell.execute_reply":"2022-08-30T16:46:45.894099Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"To train a model, we'll need `DataLoaders`, which is an object that contains a *training set* (the images used to create a model) and a *validation set* (the images used to check the accuracy of a model -- not used during training). In `fastai` we can create that easily using a `DataBlock`, and view sample images from it:","metadata":{}},{"cell_type":"code","source":"dls = DataBlock(\n    blocks=(ImageBlock, CategoryBlock), \n    get_items=get_image_files, \n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=parent_label,\n    item_tfms=[Resize(192, method='squish')]\n).dataloaders(path, bs=32)\n\ndls.show_batch(max_n=6)","metadata":{"execution":{"iopub.status.busy":"2022-08-30T16:46:59.882817Z","iopub.execute_input":"2022-08-30T16:46:59.883113Z","iopub.status.idle":"2022-08-30T16:47:05.749611Z","shell.execute_reply.started":"2022-08-30T16:46:59.883079Z","shell.execute_reply":"2022-08-30T16:47:05.747987Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"Here what each of the `DataBlock` parameters means:\n\n    blocks=(ImageBlock, CategoryBlock),\n\nThe inputs to our model are images, and the outputs are categories (in this case, \"bird\" or \"forest\").\n\n    get_items=get_image_files, \n\nTo find all the inputs to our model, run the `get_image_files` function (which returns a list of all image files in a path).\n\n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n\nSplit the data into training and validation sets randomly, using 20% of the data for the validation set.\n\n    get_y=parent_label,\n\nThe labels (`y` values) is the name of the `parent` of each file (i.e. the name of the folder they're in, which will be *bird* or *forest*).\n\n    item_tfms=[Resize(192, method='squish')]\n\nBefore training, resize each image to 192x192 pixels by \"squishing\" it (as opposed to cropping it).","metadata":{}},{"cell_type":"markdown","source":"Now we're ready to train our model. The fastest widely used computer vision model is `resnet18`. You can train this in a few minutes, even on a CPU! (On a GPU, it generally takes under 10 seconds...)\n\n`fastai` comes with a helpful `fine_tune()` method which automatically uses best practices for fine tuning a pre-trained model, so we'll use that.","metadata":{}},{"cell_type":"code","source":"learn = vision_learner(dls, resnet18, metrics=error_rate)\nlearn.fine_tune(3)","metadata":{"execution":{"iopub.status.busy":"2022-08-30T16:47:17.178234Z","iopub.execute_input":"2022-08-30T16:47:17.178858Z","iopub.status.idle":"2022-08-30T16:47:25.040002Z","shell.execute_reply.started":"2022-08-30T16:47:17.178815Z","shell.execute_reply":"2022-08-30T16:47:25.039178Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"Generally when I run this I see 100% accuracy on the validation set (although it might vary a bit from run to run).\n\n\"Fine-tuning\" a model means that we're starting with a model someone else has trained using some other dataset (called the *pretrained model*), and adjusting the weights a little bit so that the model learns to recognise your particular dataset. In this case, the pretrained model was trained to recognise photos in *imagenet*, and widely-used computer vision dataset with images covering 1000 categories) For details on fine-tuning and why it's important, check out the [free fast.ai course](https://course.fast.ai/).","metadata":{}},{"cell_type":"markdown","source":"## Step 3: Use our model (and build your own!)","metadata":{}},{"cell_type":"markdown","source":"Let's see what our model thinks about the apple fruit picture we downloaded at the start:","metadata":{}},{"cell_type":"code","source":"is_apple,_,probs = learn.predict(PILImage.create('apple.jpg'))\nprint(f\"This is an: {is_apple}.\")\nprint(f\"Probability it's an apple: {probs[0]:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2022-08-30T16:51:32.878561Z","iopub.execute_input":"2022-08-30T16:51:32.879312Z","iopub.status.idle":"2022-08-30T16:51:33.009905Z","shell.execute_reply.started":"2022-08-30T16:51:32.879273Z","shell.execute_reply":"2022-08-30T16:51:33.009105Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"Good job, resnet18. :)\n\nSo, as you see, in the space of a few years, creating computer vision classification models has gone from \"so hard it's a joke\" to \"trivially easy and free\"!\n\nIt's not just in computer vision. Thanks to deep learning, computers can now do many things which seemed impossible just a few years ago, including [creating amazing artworks](https://openai.com/dall-e-2/), and [explaining jokes](https://www.datanami.com/2022/04/22/googles-massive-new-language-model-can-explain-jokes/). It's moving so fast that even experts in the field have trouble predicting how it's going to impact society in the coming years.\n\nOne thing is clear -- it's important that we all do our best to understand this technology, because otherwise we'll get left behind!","metadata":{}},{"cell_type":"markdown","source":"Now it's your turn. Click \"Copy & Edit\" and try creating your own image classifier using your own image searches!\n\nIf you enjoyed this, please consider clicking the \"upvote\" button in the top-right -- it's very encouraging to us notebook authors to know when people appreciate our work.","metadata":{}}]}